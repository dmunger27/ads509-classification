{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "# Stopwords\n",
    "sw = nltk.corpus.stopwords.words(\"english\")\n",
    "#Punctuation\n",
    "punctuation = set(punctuation)\n",
    "def clean_text_data(text):\n",
    "    new_text = text\n",
    "    for i in new_text:\n",
    "        if i in punctuation:\n",
    "            new_text = new_text.replace(i, \" \")\n",
    "    # Remove extra white space\n",
    "    new_text = re.sub(r'\\s+', ' ', new_text)\n",
    "    # Split on whitespace\n",
    "    new_text = new_text.split()\n",
    "    # Fold to lowercase\n",
    "    for i in range(len(new_text)):\n",
    "        new_text[i] = new_text[i].lower()\n",
    "    # Remove stopwords\n",
    "    new_text = [i for i in new_text if i not in sw]\n",
    "    # Remove blank items\n",
    "    new_text = [j for j in new_text if j]\n",
    "    return new_text\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT party, text\n",
    "                            FROM conventions\n",
    "                            ''')\n",
    "for row in query_results :\n",
    "    # store the results in convention_data\n",
    "    convention_data.append([' '.join(clean_text_data(row[1])), row[0]])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['military better equipped better resourced better manned military world president trump demolished terrorist isis caliphate middle east eliminated leader al baghdadi one worldâ€™s brutal terrorists president trump took decisive action iranian terrorist mastermind qasem soleimani man responsible deaths hundreds american service men iraq nato allies failed meet commitments upheld president trump demanded parody nato members increased contributions 100 billion year natoâ€™s secretary general credits president donald j trump',\n",
       "  'Republican'],\n",
       " ['help god', 'Republican'],\n",
       " ['lost nearly 50 business china mid 2000s wondered small company like could continue compete time joe biden senator voted normalize trade china help pave way join wto even though hurting american companies like weâ€™re tenacious weâ€™re creative took risk purchased 3d printer 3d printing technology allows us things china canâ€™t',\n",
       "  'Republican'],\n",
       " ['protests st louis turn violent', 'Republican'],\n",
       " ['iâ€™m example woman given second chance life god bless god bless america good evening iâ€™m alice marie johnson told way would ever reunited family would corpse grace god compassion president donald john trump stand tonight assure iâ€™m ghost alive well importantly free 1996 began serving time prison life plus 25 years never trouble first time non violent offender wrong made decisions regret',\n",
       "  'Republican'],\n",
       " ['couple years ago diagnosed cancer absolutely terrified one first people called joe real warmth kindness call man got say made cry current president made cry itâ€™s never anything warmth kindness joe bidenâ€™s empathy genuine feel thatâ€™s president obama asked joe head cancer moonshot president obama knows know joe biden understands suffering loss sacrifice mayor pete buttigieg also knows something sacrifice volunteered deployed afghanistan returned home indiana become highly effective public servant say hi mayor pete',\n",
       "  'Democratic'],\n",
       " ['â€¦ family', 'Democratic'],\n",
       " ['sick 10 days really bad got everything besides cough recovered work month half work local county jails',\n",
       "  'Republican'],\n",
       " ['yeah every day', 'Democratic'],\n",
       " ['nobody law', 'Democratic']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 10, we have 1478 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 10\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    ret_dict = dict()\n",
    "    for i in text.split():\n",
    "        if i in fw:\n",
    "            ret_dict[i] = True\n",
    "    return(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.492\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     25.8 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     14.9 : 1.0\n",
      "                 defense = True           Republ : Democr =     14.0 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                  defund = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "Most of the informative features are weighted heavier Republican than Democrat, indicating that there are more words that distinguish an individual as a Republican than as a Democrat. The words themselves also are interesting as they can reveal policy positions and issues that the respective party finds most important. For example, the word \"china\" was used far more frequently by Republicans than Democrats, while \"votes\" was used more frequently by Democrats compared to Republicans. Overall, this analysis can help reveal what each party finds to be the most important issues of the day. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "for row in results:\n",
    "    tweet_data.append([' '.join(clean_text_data(row[2].decode(\"utf-8\", \"ignore\"))), row[1]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earlier today spoke house floor abt protecting health care women praised ppmarmonte work central coast https co wqgtrzt7vv\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: go tribe rallytogether https co 0nxutfl9l5\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: apparently trump thinks easy students overwhelmed crushing burden debt pay student loans trumpbudget https co ckyqo5t0qh\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: weâ€™re grateful first responders rescue personnel firefighters police volunteers working tirelessly keep people safe provide much needed help putting lives line https co ezpv0vmiz3\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: letâ€™s make even greater kag ðŸ‡ºðŸ‡¸ https co y9qozd5l2z\n",
      "Actual party is Republican and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: 1hr cavs tie series 2 2 allin216 repbarbaralee scared roadtovictory\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: congrats belliottsd new gig sd city hall glad continue serveâ€¦ https co fkvmw3cqdi\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: really close 3500 raised toward match right whoot thatâ€™s 7000 non math majors room ðŸ˜‚ help us get https co tu34c472sd https co qsdqkypsmc\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: today comment period potusâ€™s plan expand offshore drilling opened public 60 days march 9 share oppose proposed program directly trump administration comments made email mail https co baaymejxqn\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: celebrated icseastlaâ€™s 22 years eastside commitment amp saluted community leaders last nightâ€™s awards dinner https co 7v7gh8givb\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet, party in tweet_data_sample :\n",
    "    estimated_party = classifier.classify(conv_features(tweet, feature_words))\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = classifier.classify(conv_features(tweet, feature_words))\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3104, 'Democratic': 1174}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4075, 'Democratic': 1649})})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "The primary issue with the model is it primarily predicts Republican, even for Democratic tweets. This could be centered around the previous comment, that Republicans have more distinguishing words than Democrats in speeches. Since a word appears to be more likely to indicate an individual is Republican, the model overpredicts Republican. Naive Bayes may not be the best model for this data. It may also be useful to gather more speech data to train the data on, as it may reveal more Democratic weighted words. Data from previous conventions, for example, may be more insightful and produce more accurate results. The word usage and content of speeches would likely be impacted by which party currently controlled the White House, the House, and the Senate. Current political climate, world issues, and more would also likely impact the word choice in speeches. Therefore, selecting only one year may give the best representation of word choice by party. It should also be considered that conditions in 2020 were vastly different from those in 2018 due simply to the COVID-19 pandemic. This would surely impact word choice as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
